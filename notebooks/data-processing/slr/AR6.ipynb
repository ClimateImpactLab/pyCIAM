{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.9/site-packages/dask_gateway/client.py:21: FutureWarning: format_bytes is deprecated and will be removed in a future release. Please use dask.utils.format_bytes instead.\n",
      "  from distributed.utils import LoopRunner, format_bytes\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import shared\n",
    "import xarray as xr\n",
    "from shared import open_dataset, save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and aggregate AR6 FACTS projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing gs://rhg-data/impactlab-rhg/coastal/ciam_paper/data/raw/slr/ar6/regional/total_ssp126_low_confidence_values.nc...\n",
      "Processing gs://rhg-data/impactlab-rhg/coastal/ciam_paper/data/raw/slr/ar6/regional/total_ssp245_low_confidence_values.nc...\n",
      "Processing gs://rhg-data/impactlab-rhg/coastal/ciam_paper/data/raw/slr/ar6/regional/total_ssp585_low_confidence_values.nc...\n",
      "Processing gs://rhg-data/impactlab-rhg/coastal/ciam_paper/data/raw/slr/ar6/regional/total_ssp119_medium_confidence_values.nc...\n",
      "Processing gs://rhg-data/impactlab-rhg/coastal/ciam_paper/data/raw/slr/ar6/regional/total_ssp126_medium_confidence_values.nc...\n",
      "Processing gs://rhg-data/impactlab-rhg/coastal/ciam_paper/data/raw/slr/ar6/regional/total_ssp245_medium_confidence_values.nc...\n",
      "Processing gs://rhg-data/impactlab-rhg/coastal/ciam_paper/data/raw/slr/ar6/regional/total_ssp370_medium_confidence_values.nc...\n",
      "Processing gs://rhg-data/impactlab-rhg/coastal/ciam_paper/data/raw/slr/ar6/regional/total_ssp585_medium_confidence_values.nc...\n",
      "Processing gs://rhg-data/impactlab-rhg/coastal/ciam_paper/data/raw/slr/ar6/regional/verticallandmotion_ssp126_low_confidence_values.nc...\n",
      "Processing gs://rhg-data/impactlab-rhg/coastal/ciam_paper/data/raw/slr/ar6/regional/verticallandmotion_ssp245_low_confidence_values.nc...\n",
      "Processing gs://rhg-data/impactlab-rhg/coastal/ciam_paper/data/raw/slr/ar6/regional/verticallandmotion_ssp585_low_confidence_values.nc...\n",
      "Processing gs://rhg-data/impactlab-rhg/coastal/ciam_paper/data/raw/slr/ar6/regional/verticallandmotion_ssp119_medium_confidence_values.nc...\n",
      "Processing gs://rhg-data/impactlab-rhg/coastal/ciam_paper/data/raw/slr/ar6/regional/verticallandmotion_ssp126_medium_confidence_values.nc...\n",
      "Processing gs://rhg-data/impactlab-rhg/coastal/ciam_paper/data/raw/slr/ar6/regional/verticallandmotion_ssp245_medium_confidence_values.nc...\n",
      "Processing gs://rhg-data/impactlab-rhg/coastal/ciam_paper/data/raw/slr/ar6/regional/verticallandmotion_ssp370_medium_confidence_values.nc...\n",
      "Processing gs://rhg-data/impactlab-rhg/coastal/ciam_paper/data/raw/slr/ar6/regional/verticallandmotion_ssp585_medium_confidence_values.nc...\n"
     ]
    }
   ],
   "source": [
    "all_ds = []\n",
    "global_ds = []\n",
    "global_ssps = []\n",
    "\n",
    "\n",
    "def open_and_convert(ds_path):\n",
    "    out = open_dataset(ds_path)\n",
    "    out[\"sea_level_change\"] = (\n",
    "        out.sea_level_change.pint.quantify().pint.to(\"meters\").pint.dequantify()\n",
    "    )\n",
    "    return out\n",
    "\n",
    "\n",
    "for kind in [\"total\", \"verticallandmotion\"]:\n",
    "    ds = []\n",
    "    this_ssps = []\n",
    "    for conf in [\"low\", \"medium\"]:\n",
    "        for ds_path in (shared.DIR_SLR_AR6_RAW / \"regional\").glob(f\"{kind}_*_{conf}_*\"):\n",
    "            print(f\"Processing {ds_path}...\")\n",
    "            this_ssp = ds_path.name.split(\"_\")[1]\n",
    "            ssp_conf = f\"{this_ssp}_{conf}\"\n",
    "            ds.append(open_and_convert(ds_path))\n",
    "            this_ssps.append(ssp_conf)\n",
    "            if kind == \"total\":\n",
    "                global_ds.append(\n",
    "                    open_and_convert(shared.DIR_SLR_AR6_RAW / \"global\" / ds_path.name)\n",
    "                )\n",
    "                global_ssps.append(ssp_conf)\n",
    "    all_ds.append(\n",
    "        xr.concat(ds, pd.Index(this_ssps, name=\"scenario\"), data_vars=\"different\")\n",
    "    )\n",
    "\n",
    "# for some reason the VLM dataset has an entry for 2005 that is all 0s, while other\n",
    "# datasets just don't have 2005 b/c it is the assumed basline\n",
    "assert (all_ds[1].sea_level_change.sel(years=2005) == 0).all()\n",
    "all_ds[1] = all_ds[1].sel(years=slice(2006, None))\n",
    "\n",
    "global_ds = (\n",
    "    xr.concat(global_ds, pd.Index(global_ssps, name=\"scenario\"), data_vars=\"different\")\n",
    "    .squeeze(drop=True)\n",
    "    .drop_vars([\"lon\", \"lat\"])\n",
    "    .sea_level_change\n",
    ")\n",
    "\n",
    "# handle floating point matching errors on the quantile dimension\n",
    "global_ds[\"quantiles\"] = all_ds[0].quantiles\n",
    "all_ds[1][\"quantiles\"] = all_ds[0].quantiles\n",
    "\n",
    "all_ds = xr.Dataset(\n",
    "    {\n",
    "        \"lsl_msl05\": all_ds[0].sea_level_change,\n",
    "        \"lsl_ncc_msl05\": all_ds[1].sea_level_change,\n",
    "        \"gsl_msl05\": global_ds,\n",
    "        \"lon\": all_ds[1].lon,\n",
    "        \"lat\": all_ds[0].lat,\n",
    "    }\n",
    ")\n",
    "\n",
    "# drop locations with NaN values in the time period we're interested in\n",
    "valid = (\n",
    "    all_ds[[\"lsl_msl05\", \"lsl_ncc_msl05\"]]\n",
    "    .sel(years=slice(2100))\n",
    "    .notnull()\n",
    "    .all([\"scenario\", \"quantiles\", \"years\"])\n",
    "    .to_array(\"tmp\")\n",
    "    .all(\"tmp\")\n",
    ")\n",
    "all_ds = all_ds.sel(locations=valid)\n",
    "\n",
    "all_ds = all_ds.rename(\n",
    "    {\"years\": \"year\", \"quantiles\": \"quantile\", \"locations\": \"site_id\"}\n",
    ")\n",
    "\n",
    "# we generally allow +180 but not -180\n",
    "all_ds[\"lon\"] = all_ds.lon.where(all_ds.lon != -180, 180)\n",
    "\n",
    "# ensure no locations have missing values\n",
    "assert all_ds.sel(year=slice(2100)).notnull().all().to_array().all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in all_ds.variables:\n",
    "    all_ds[v].encoding.clear()\n",
    "save(all_ds.chunk({\"site_id\": 100}), shared.PATH_SLR_AR6, mode=\"w\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
